"""
Web3 Daily Digest - Telegram Bot Entry Point

Main application file that initializes the bot, sets up handlers,
and configures scheduled tasks for daily digest delivery.

Reference: python-telegram-bot v22.x with built-in JobQueue
"""
import asyncio
import logging
from logging.handlers import TimedRotatingFileHandler
import os
import sys
import signal
import atexit
from datetime import time, datetime, timedelta
from typing import Dict, Any
from zoneinfo import ZoneInfo

from telegram import BotCommand, InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import (
    Application,
    CommandHandler,
    CallbackQueryHandler,
    ContextTypes,
    MessageHandler,
    filters,
)

from config import (
    TELEGRAM_BOT_TOKEN, PUSH_HOUR, PUSH_MINUTE, DATA_DIR,
    LOG_ROTATE_DAYS, LOG_BACKUP_COUNT, MAX_DIGEST_ITEMS, CONCURRENT_USERS,
    PREFETCH_INTERVAL_HOURS, PREFETCH_START_HOUR, ADMIN_TELEGRAM_IDS
)
from services.digest_processor import process_single_user
from utils.telegram_utils import safe_answer_callback_query
from handlers.start import get_start_handler, get_start_callbacks
from handlers.feedback import get_feedback_handlers
from handlers.settings import get_settings_handler, get_settings_callbacks
from handlers.sources import get_sources_handler, get_sources_callbacks
from handlers.admin import get_admin_handlers


# ============ Logging Configuration ============

# Create logs directory
LOGS_DIR = os.path.join(DATA_DIR, "logs")
os.makedirs(LOGS_DIR, exist_ok=True)


class HeartbeatFilter(logging.Filter):
    """Filter out noisy heartbeat/polling log messages."""

    # Patterns to filter out
    NOISE_PATTERNS = [
        "HTTP Request: POST https://api.telegram.org/bot",
        "getUpdates",
        "Got response",
        "Entering:",
        "Exiting:",
        "No updates to fetch",
        "Network loop",
    ]

    def filter(self, record: logging.LogRecord) -> bool:
        msg = record.getMessage()
        for pattern in self.NOISE_PATTERNS:
            if pattern in msg:
                return False
        return True


log_formatter = logging.Formatter(
    "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)

# Console handler (with heartbeat filter)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(log_formatter)
console_handler.addFilter(HeartbeatFilter())

# Timed rotating file handler: rotate every N days, keep 30 backups
# ÊåâÂ§©ËΩÆËΩ¨Êó•ÂøóÔºåLOG_ROTATE_DAYS ÊéßÂà∂Âá†Â§©ËΩÆËΩ¨‰∏ÄÊ¨°ÔºåLOG_BACKUP_COUNT ÊéßÂà∂‰øùÁïôÊï∞Èáè
file_handler = TimedRotatingFileHandler(
    os.path.join(LOGS_DIR, "bot.log"),
    when="D",                        # ÊåâÂ§©
    interval=LOG_ROTATE_DAYS,        # ÊØè N Â§©ËΩÆËΩ¨
    backupCount=LOG_BACKUP_COUNT,    # ‰øùÁïô N ‰∏™Â§á‰ªΩÔºàÈªòËÆ§30Â§©Ôºâ
    encoding="utf-8"
)
file_handler.setFormatter(log_formatter)
file_handler.addFilter(HeartbeatFilter())

logging.basicConfig(
    level=logging.INFO,
    handlers=[console_handler, file_handler]
)

# Reduce noise from third-party libraries
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)
logging.getLogger("telegram.ext").setLevel(logging.WARNING)
logging.getLogger("apscheduler").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

# Force reload environment variables and update config
from dotenv import load_dotenv
import config

env_path = os.path.join(os.path.dirname(__file__), '.env')
if os.path.exists(env_path):
    load_dotenv(env_path, override=True)
    logger.info(f"Force reloaded .env from {env_path}")

# Update config variable (support multiple admins)
_admin_ids_str = os.getenv("ADMIN_TELEGRAM_IDS", "") or os.getenv("ADMIN_TELEGRAM_ID", "")
config.ADMIN_TELEGRAM_IDS = [id.strip() for id in _admin_ids_str.split(",") if id.strip()]
config.ADMIN_TELEGRAM_ID = config.ADMIN_TELEGRAM_IDS[0] if config.ADMIN_TELEGRAM_IDS else ""
logger.info(f"Admin IDs configured: {len(config.ADMIN_TELEGRAM_IDS)} admin(s)")


async def daily_digest_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Scheduled job to generate and send daily digest to all users.
    Runs at configured time (default: 9:00 AM Beijing Time).
    Uses concurrent processing for better performance with pre-fetching optimization.
    """
    from utils.json_storage import get_users, get_user_sources
    from services.rss_fetcher import fetch_all_sources

    logger.info("Starting daily digest generation...")

    today = datetime.now().strftime("%Y-%m-%d")

    try:
        # Get all users
        users = get_users()
        if not users:
            logger.warning("No users registered, skipping digest")
            return

        # ===== Pre-fetch optimization: Collect all unique sources =====
        logger.info("Collecting all user sources...")
        all_sources = {}  # {category: {name: url}}

        for user in users:
            telegram_id = user.get("telegram_id")
            if not telegram_id:
                continue

            user_sources = get_user_sources(telegram_id)
            # Merge into global sources dict
            for category, sources in user_sources.items():
                if category not in all_sources:
                    all_sources[category] = {}
                for name, url in sources.items():
                    if url and name not in all_sources[category]:
                        all_sources[category][name] = url

        # Count unique sources
        unique_sources_count = sum(len(sources) for sources in all_sources.values())
        logger.info(f"Found {unique_sources_count} unique RSS sources across {len(users)} users")

        # Batch fetch all sources once (shared by all users)
        logger.info("Pre-fetching all RSS sources...")
        global_raw_content = await fetch_all_sources(
            hours_back=24,
            sources=all_sources
        )
        logger.info(f"Pre-fetched {len(global_raw_content)} total items")
        # ===== Pre-fetch complete =====

        logger.info(f"Processing {len(users)} users with concurrency={CONCURRENT_USERS}")

        # Concurrent processing with semaphore to limit concurrency
        semaphore = asyncio.Semaphore(CONCURRENT_USERS)

        async def process_with_limit(user):
            async with semaphore:
                # Pass global data to avoid duplicate fetching
                return await process_single_user(context, user, today, global_raw_content)

        # Process all users concurrently
        results = await asyncio.gather(
            *[process_with_limit(user) for user in users],
            return_exceptions=True  # Single user failure doesn't affect others
        )

        # Collect statistics
        success_count = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "success")
        error_count = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "error")
        skipped_count = sum(1 for r in results if isinstance(r, dict) and r.get("status") == "skipped")
        exception_count = sum(1 for r in results if isinstance(r, Exception))

        logger.info(
            f"Daily digest complete: {success_count} success, {error_count} errors, "
            f"{skipped_count} skipped, {exception_count} exceptions"
        )

        # Log any exceptions that occurred
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"User {users[i].get('telegram_id')} raised exception: {result}")

    except Exception as e:
        logger.error(f"Daily digest job failed: {e}", exc_info=True)


async def test_fetch_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Hidden /test command - manually trigger digest for the current user only."""
    from utils.json_storage import get_user, get_user_sources
    from services.rss_fetcher import fetch_all_sources
    
    user = update.effective_user
    telegram_id = str(user.id)
    today = datetime.now().strftime("%Y-%m-%d")

    await update.message.reply_text("Ê≠£Âú®‰∏∫‰Ω†ÁîüÊàêÁÆÄÊä•...")

    try:
        # Get current user's data
        user_data = get_user(telegram_id)
        if not user_data:
            await update.message.reply_text("Êú™ÊâæÂà∞‰Ω†ÁöÑÁî®Êà∑Êï∞ÊçÆÔºåËØ∑ÂÖàÂÆåÊàê /start ËÆæÁΩÆ„ÄÇ")
            return
        
        # Fetch sources for this user only
        user_sources = get_user_sources(telegram_id)
        if not user_sources:
            await update.message.reply_text("‰Ω†ËøòÊ≤°ÊúâÈÖçÁΩÆ‰ø°ÊÅØÊ∫êÔºåËØ∑‰ΩøÁî® /sources Ê∑ªÂä†„ÄÇ")
            return
        
        # Add sources to user_data for process_single_user
        user_data["sources"] = user_sources
        
        logger.info(f"Test command: Processing user {telegram_id}")
        
        # Fetch RSS content for this user
        raw_content = await fetch_all_sources(
            hours_back=24,
            sources=user_sources
        )
        logger.info(f"Test command: Fetched {len(raw_content)} items for user {telegram_id}")
        
        # Process single user
        result = await process_single_user(context, user_data, today, raw_content)
        
        if result.get("status") == "success":
            await update.message.reply_text("ÁÆÄÊä•Â∑≤ÂèëÈÄÅÔºÅ")
        elif result.get("status") == "skipped":
            await update.message.reply_text(f"Ë∑≥Ëøá: {result.get('reason', 'Êú™Áü•ÂéüÂõ†')}")
        else:
            await update.message.reply_text(f"Â§ÑÁêÜÂ§±Ë¥•: {result.get('error', 'Êú™Áü•ÈîôËØØ')[:100]}")
            
    except Exception as e:
        logger.error(f"Test fetch failed for user {telegram_id}: {e}", exc_info=True)
        await update.message.reply_text(f"ÊäìÂèñÂ§±Ë¥•: {str(e)[:100]}")


async def test_profile_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Hidden /testprofile command - manually trigger profile update from feedback."""
    from services.profile_updater import update_all_user_profiles

    await update.message.reply_text("Ê≠£Âú®Êõ¥Êñ∞Áî®Êà∑ÁîªÂÉè...")

    try:
        await update_all_user_profiles()
        await update.message.reply_text("ÁîªÂÉèÊõ¥Êñ∞ÂÆåÊàê„ÄÇ")
    except Exception as e:
        logger.error(f"Test profile update failed: {e}")
        await update.message.reply_text(f"Êõ¥Êñ∞Â§±Ë¥•: {str(e)[:100]}")


async def test_prefetch_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Hidden /testprefetch command - manually trigger prefetch and show stats."""
    from services.rss_fetcher import prefetch_all_user_sources
    from utils.json_storage import get_prefetch_cache
    from datetime import datetime

    await update.message.reply_text("üîÑ Ê≠£Âú®ÊâßË°åÈ¢ÑÊäìÂèñÊµãËØï...")

    try:
        # ÊâßË°åÈ¢ÑÊäìÂèñ
        stats = await prefetch_all_user_sources()

        # Ëé∑ÂèñÁºìÂ≠òÁä∂ÊÄÅ
        today = datetime.now().strftime("%Y-%m-%d")
        cache = get_prefetch_cache(today)

        result_text = f"""‚úÖ È¢ÑÊäìÂèñÊµãËØïÂÆåÊàê

üìä Êú¨Ê¨°ÊäìÂèñÁªüËÆ°Ôºö
‚Ä¢ Áî®Êà∑Êï∞: {stats.get('users_count', 0)}
‚Ä¢ ‰ø°ÊÅØÊ∫êÊï∞: {stats.get('sources_count', 0)}
‚Ä¢ Êñ∞Â¢ûÊù°ÁõÆ: {stats.get('new_items', 0)}
‚Ä¢ ÈáçÂ§çË∑≥Ëøá: {stats.get('duplicates', 0)}

üì¶ ÂΩìÊó•ÁºìÂ≠òÁä∂ÊÄÅÔºö
‚Ä¢ Á¥ØËÆ°Êù°ÁõÆ: {len(cache.get('items', []))}
‚Ä¢ ÂéªÈáç ID Êï∞: {len(cache.get('seen_ids', []))}
‚Ä¢ ÊäìÂèñÊ¨°Êï∞: {cache.get('fetch_count', 0)}
‚Ä¢ ÊúÄÂêéÊäìÂèñ: {cache.get('last_fetch', 'N/A')[:19] if cache.get('last_fetch') else 'N/A'}

üí° ÊèêÁ§∫Ôºö
‚Ä¢ Â¶ÇÊûú"Êñ∞Â¢ûÊù°ÁõÆ"‰∏∫ 0 ‰∏î"ÈáçÂ§çË∑≥Ëøá"ÊúâÊï∞ÂÄºÔºåËØ¥ÊòéÂéªÈáçÊ≠£Â∏∏Â∑•‰Ωú
‚Ä¢ Â§öÊ¨°ÊâßË°åÊ≠§ÂëΩ‰ª§ÔºåËßÇÂØü"Á¥ØËÆ°Êù°ÁõÆ"ÊòØÂê¶Â¢ûÂä†ÔºàÊúâÊñ∞Êé®ÊñáÊó∂Ôºâ
‚Ä¢ ‰ΩøÁî® /test ÂëΩ‰ª§ÂèØÊµãËØïÂÆåÊï¥Êé®ÈÄÅÊµÅÁ®ã"""

        await update.message.reply_text(result_text)

    except Exception as e:
        logger.error(f"Test prefetch failed: {e}", exc_info=True)
        await update.message.reply_text(f"‚ùå È¢ÑÊäìÂèñÂ§±Ë¥•: {str(e)[:200]}")


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle /help command."""
    keyboard = [
        [
            InlineKeyboardButton("‰∏ªËèúÂçï", callback_data="back_to_start"),
            InlineKeyboardButton("ËÆæÁΩÆ", callback_data="update_preferences"),
        ],
        [InlineKeyboardButton("‰ø°ÊÅØÊ∫ê", callback_data="manage_sources")],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    help_text = f"""Â∏ÆÂä©
{'‚îÄ' * 24}

ÂëΩ‰ª§Ôºö
  /start     ‰∏ªËèúÂçï
  /settings  ÂÅèÂ•ΩËÆæÁΩÆ
  /sources   ‰ø°ÊÅØÊ∫êÁÆ°ÁêÜ
  /stats     Êü•ÁúãÁªüËÆ°
  /help      Â∏ÆÂä©‰ø°ÊÅØ

{'‚îÄ' * 24}

ÂäüËÉΩËØ¥ÊòéÔºö

ÊØèÊó•ÁÆÄÊä•
  ÊØèÂ§© {PUSH_HOUR:02d}:{PUSH_MINUTE:02d}ÔºàÂåó‰∫¨Êó∂Èó¥ÔºâËá™Âä®Êé®ÈÄÅ„ÄÇ
  ÂÜÖÂÆπÂàÜ‰∏∫„Äå‰ªäÊó•ÂøÖÁúã„Äç„ÄåÊé®Ëçê„Äç„ÄåÂÖ∂‰ªñ„Äç‰∏â‰∏™ÊùøÂùó„ÄÇ
  AI Ê†πÊçÆ‰Ω†ÁöÑÂÅèÂ•ΩÊô∫ËÉΩÁ≠õÈÄâ 15-30 Êù°Á≤æÈÄâÂÜÖÂÆπ„ÄÇ

ÂÅèÂ•ΩËÆæÁΩÆ (/settings)
  Êü•ÁúãÊàñÊõ¥Êñ∞‰Ω†ÁöÑ Web3 ÂÖ¥Ë∂£ÂÅèÂ•Ω„ÄÇ
  AI ‰ºöÊ†πÊçÆÂÅèÂ•Ω‰∏™ÊÄßÂåñÁ≠õÈÄâÊØèÊó•ÁÆÄÊä•„ÄÇ

‰ø°ÊÅØÊ∫êÁÆ°ÁêÜ (/sources)
  Ê∑ªÂä†/Âà†Èô§‰Ω†ÂÖ≥Ê≥®ÁöÑ Twitter Ë¥¶Âè∑ÊàñÁΩëÁ´ô RSS„ÄÇ
  ÊîØÊåÅËá™ÂÆö‰πâ‰∏™‰∫∫‰ø°ÊÅØÊ∫ê„ÄÇ

ÁªüËÆ° (/stats)
  Êü•Áúã‰Ω†ÁöÑÊ≥®ÂÜåÊó∂Èó¥„ÄÅÂèçÈ¶àÂéÜÂè≤„ÄÅÊª°ÊÑèÂ∫¶Ë∂ãÂäø„ÄÇ

ÂèçÈ¶àÊú∫Âà∂
  ÊØèÊù°Êé®ÈÄÅÊ∂àÊÅØÈÉΩÊúâÂèçÈ¶àÊåâÈíÆÔºàüëç/üëéÔºâ„ÄÇ
  ‰Ω†ÁöÑÂèçÈ¶à‰ºöË¢´Êî∂ÈõÜÂπ∂Âú®ÊØèÊó•ÂáåÊô®ÊâπÈáèÊõ¥Êñ∞ÂÅèÂ•ΩÁîªÂÉèÔºå
  Ê¨°Êó•Êé®ÈÄÅÂ∞Ü‰ΩìÁé∞‰Ω†ÁöÑÊúÄÊñ∞ÂÅèÂ•Ω„ÄÇ

{'‚îÄ' * 24}

ÊúâÈóÆÈ¢òÔºü‰ΩøÁî®‰∏äÊñπÂëΩ‰ª§Êàñ‰∏ªËèúÂçïÊìç‰Ωú„ÄÇ"""

    await update.message.reply_text(help_text, reply_markup=reply_markup)


async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle /stats command - show user statistics."""
    from services.profile_updater import analyze_feedback_trends
    from utils.json_storage import get_user

    def _translate_trend(trend: str) -> str:
        """Translate trend text to Chinese."""
        translations = {
            "improving": "ÊîπÂñÑ‰∏≠",
            "declining": "‰∏ãÈôç‰∏≠",
            "stable": "Á®≥ÂÆö",
            "no_data": "ÊöÇÊó†Êï∞ÊçÆ",
        }
        return translations.get(trend, trend.replace('_', ' '))

    user = update.effective_user
    telegram_id = str(user.id)

    db_user = get_user(telegram_id)
    if not db_user:
        await update.message.reply_text(
            "‰Ω†ËøòÊ≤°ÊúâÊ≥®ÂÜå„ÄÇËØ∑‰ΩøÁî® /start ÂºÄÂßã„ÄÇ"
        )
        return

    # Get feedback trends
    trends = await analyze_feedback_trends(telegram_id, days=30)

    stats_text = f"""‰Ω†ÁöÑÁªüËÆ°
{'‚îÄ' * 24}

Ê≥®ÂÜåÊó∂Èó¥Ôºö{db_user.get('created', 'Êú™Áü•')[:10]}

ÊúÄËøë 30 Â§©
  ÂèçÈ¶àÊ¨°Êï∞         {trends['total_feedbacks']}
  Ê≠£Èù¢ËØÑ‰ª∑         {trends['positive_count']}
  Ë¥üÈù¢ËØÑ‰ª∑         {trends['negative_count']}
  Êª°ÊÑèÂ∫¶           {trends['positive_rate']:.0%}
  Ë∂ãÂäø             {_translate_trend(trends['trend'])}
{f"  ‰∏ªË¶ÅÈóÆÈ¢ò         {', '.join(trends['common_issues'][:2])}" if trends['common_issues'] else ""}

{'‚îÄ' * 24}

‰ΩøÁî® /settings Ë∞ÉÊï¥ÂÅèÂ•ΩËÆæÁΩÆ„ÄÇ"""

    keyboard = [
        [
            InlineKeyboardButton("Êõ¥Êñ∞ÂÅèÂ•Ω", callback_data="settings_update"),
            InlineKeyboardButton("‰∏ªËèúÂçï", callback_data="back_to_start"),
        ],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    await update.message.reply_text(stats_text, reply_markup=reply_markup)


async def post_init(application: Application) -> None:
    """Post-initialization callback to set up scheduled jobs and bot commands."""
    # Set bot commands menu (only show user-facing commands)
    # Debug commands (/test, /testprofile) are hidden from menu but still functional
    commands = [
        BotCommand("start", "‰∏ªËèúÂçï"),
        BotCommand("help", "Â∏ÆÂä©‰ø°ÊÅØ"),
        BotCommand("settings", "ÂÅèÂ•ΩËÆæÁΩÆ"),
        BotCommand("sources", "‰ø°ÊÅØÊ∫êÁÆ°ÁêÜ"),
        BotCommand("stats", "Êü•ÁúãÁªüËÆ°"),
    ]
    await application.bot.set_my_commands(commands)
    logger.info("Bot commands menu set successfully")

    # Get timezone for Beijing
    beijing_tz = ZoneInfo("Asia/Shanghai")

    # Schedule daily digest
    push_time = time(hour=PUSH_HOUR, minute=PUSH_MINUTE, tzinfo=beijing_tz)

    application.job_queue.run_daily(
        callback=daily_digest_job,
        time=push_time,
        name="daily_digest"
    )

    logger.info(f"Scheduled daily digest at {PUSH_HOUR:02d}:{PUSH_MINUTE:02d} Beijing Time")

    # Run profile updates 30 minutes before daily digest push
    # This ensures the latest feedback is incorporated into today's filtering
    profile_update_hour = PUSH_HOUR if PUSH_MINUTE >= 30 else (PUSH_HOUR - 1) % 24
    profile_update_minute = (PUSH_MINUTE - 30) % 60
    profile_update_time = time(hour=profile_update_hour, minute=profile_update_minute, tzinfo=beijing_tz)
    application.job_queue.run_daily(
        callback=profile_update_job,
        time=profile_update_time,
        name="profile_update"
    )
    logger.info(f"Scheduled profile update at {profile_update_hour:02d}:{profile_update_minute:02d} Beijing Time (30 min before push)")

    # Run data cleanup at 00:30 daily
    # ÊØèÊó• 00:30 Ê∏ÖÁêÜËøáÊúüÊï∞ÊçÆÊñá‰ª∂
    cleanup_time = time(hour=0, minute=30, tzinfo=beijing_tz)
    application.job_queue.run_daily(
        callback=data_cleanup_job,
        time=cleanup_time,
        name="data_cleanup"
    )

    logger.info("Scheduled data cleanup at 00:30 Beijing Time")

    # Schedule prefetch jobs (if enabled)
    # È¢ÑÊäìÂèñ‰ªªÂä°ÔºöÊØèÈöî PREFETCH_INTERVAL_HOURS Â∞èÊó∂ÊâßË°å‰∏ÄÊ¨°
    if PREFETCH_INTERVAL_HOURS > 0:
        # ËÆ°ÁÆóÈ¢ÑÊäìÂèñÊó∂Èó¥ÁÇπ
        prefetch_times = []
        hour = PREFETCH_START_HOUR
        while hour < 24:
            prefetch_times.append(hour)
            hour += PREFETCH_INTERVAL_HOURS

        # ‰∏∫ÊØè‰∏™Êó∂Èó¥ÁÇπÂàõÂª∫ÂÆöÊó∂‰ªªÂä°
        for i, prefetch_hour in enumerate(prefetch_times):
            prefetch_time = time(hour=prefetch_hour, minute=0, tzinfo=beijing_tz)
            application.job_queue.run_daily(
                callback=prefetch_job,
                time=prefetch_time,
                name=f"prefetch_{prefetch_hour:02d}"
            )

        # ÂêåÊó∂Âú®Êé®ÈÄÅÂâç 30 ÂàÜÈíü‰πüÊâßË°å‰∏ÄÊ¨°È¢ÑÊäìÂèñÔºåÁ°Æ‰øùÊï∞ÊçÆÊúÄÊñ∞
        pre_push_hour = (PUSH_HOUR - 1) % 24 if PUSH_HOUR > 0 else 23
        pre_push_time = time(hour=pre_push_hour, minute=30, tzinfo=beijing_tz)
        application.job_queue.run_daily(
            callback=prefetch_job,
            time=pre_push_time,
            name="prefetch_pre_push"
        )

        logger.info(
            f"Scheduled prefetch jobs at hours: {prefetch_times} + {pre_push_hour:02d}:30 (pre-push) Beijing Time"
        )

        # ÂêØÂä®Êó∂Á´ãÂç≥ÊâßË°å‰∏ÄÊ¨°È¢ÑÊäìÂèñÔºàÂºÇÊ≠•Ôºâ
        application.job_queue.run_once(
            callback=prefetch_job,
            when=10,  # 10 ÁßíÂêéÊâßË°å
            name="prefetch_startup"
        )
        logger.info("Scheduled startup prefetch in 10 seconds")
    else:
        logger.info("Prefetch disabled (PREFETCH_INTERVAL_HOURS=0)")


async def profile_update_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """Scheduled job to update user profiles based on feedback."""
    from services.profile_updater import update_all_user_profiles

    logger.info("Running scheduled profile update...")
    try:
        await update_all_user_profiles()
        logger.info("Profile update complete")
    except Exception as e:
        logger.error(f"Profile update failed: {e}")


async def data_cleanup_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Scheduled job to clean up old data files.

    ÊØèÊó•ÂÆöÊó∂Ê∏ÖÁêÜËøáÊúüÊï∞ÊçÆÔºö
    - raw_content: ‰øùÁïô RAW_CONTENT_RETENTION_DAYS Â§©
    - daily_stats: ‰øùÁïô DAILY_STATS_RETENTION_DAYS Â§©
    - feedback: ‰øùÁïô FEEDBACK_RETENTION_DAYS Â§©
    - prefetch_cache: ‰øùÁïô 2 Â§©
    """
    from utils.json_storage import cleanup_old_data, cleanup_prefetch_cache

    logger.info("Running scheduled data cleanup...")
    try:
        results = cleanup_old_data()
        # Ê∏ÖÁêÜÈ¢ÑÊäìÂèñÁºìÂ≠òÔºà‰øùÁïô 2 Â§©Ôºâ
        prefetch_deleted = cleanup_prefetch_cache(retention_days=2)
        results["prefetch_cache"] = prefetch_deleted

        total = sum(results.values())
        if total > 0:
            logger.info(
                f"Data cleanup complete: deleted {results['raw_content']} raw_content, "
                f"{results['daily_stats']} daily_stats, {results['feedback']} feedback, "
                f"{results['prefetch_cache']} prefetch_cache files"
            )
        else:
            logger.info("Data cleanup complete: no expired files to delete")
    except Exception as e:
        logger.error(f"Data cleanup failed: {e}")


async def prefetch_job(context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    ÂÆöÊó∂È¢ÑÊäìÂèñ‰ªªÂä°„ÄÇ

    ÊØèÈöî PREFETCH_INTERVAL_HOURS Â∞èÊó∂ÊâßË°å‰∏ÄÊ¨°Ôºå
    ÊäìÂèñÊâÄÊúâÁî®Êà∑ÁöÑ RSS Ê∫êÂπ∂‰øùÂ≠òÂà∞ÁºìÂ≠òÔºàËá™Âä®ÂéªÈáçÔºâ„ÄÇ

    Ëß£ÂÜ≥ RSS.app Âè™ËøîÂõûÊúÄËøë 25 Êù°ÂÜÖÂÆπÁöÑÈóÆÈ¢ò„ÄÇ
    """
    from services.rss_fetcher import prefetch_all_user_sources

    logger.info("Running scheduled prefetch job...")
    try:
        stats = await prefetch_all_user_sources()
        logger.info(
            f"Prefetch job complete: {stats.get('new_items', 0)} new items, "
            f"{stats.get('total_items', 0)} total cached from {stats.get('sources_count', 0)} sources"
        )
    except Exception as e:
        logger.error(f"Prefetch job failed: {e}", exc_info=True)


async def error_handler(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle errors in the bot."""
    logger.error(f"Exception while handling an update: {context.error}", exc_info=context.error)


async def noop_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle noop callback for already-feedback items."""
    query = update.callback_query
    await safe_answer_callback_query(query, "Â∑≤ÁªèÂèçÈ¶àËøá‰∫Ü", show_alert=False)


async def show_help_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle show_help callback from unknown message handler."""
    query = update.callback_query
    await safe_answer_callback_query(query)

    keyboard = [
        [
            InlineKeyboardButton("‰∏ªËèúÂçï", callback_data="back_to_start"),
            InlineKeyboardButton("ËÆæÁΩÆ", callback_data="update_preferences"),
        ],
        [InlineKeyboardButton("‰ø°ÊÅØÊ∫ê", callback_data="manage_sources")],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)

    help_text = f"""Â∏ÆÂä©
{'‚îÄ' * 24}

ÂëΩ‰ª§Ôºö
  /start     ‰∏ªËèúÂçï
  /settings  ÂÅèÂ•ΩËÆæÁΩÆ
  /sources   ‰ø°ÊÅØÊ∫êÁÆ°ÁêÜ
  /stats     Êü•ÁúãÁªüËÆ°
  /help      Â∏ÆÂä©‰ø°ÊÅØ

{'‚îÄ' * 24}

ÂäüËÉΩËØ¥ÊòéÔºö

ÊØèÊó•ÁÆÄÊä•
  ÊØèÂ§© {PUSH_HOUR:02d}:{PUSH_MINUTE:02d}ÔºàÂåó‰∫¨Êó∂Èó¥ÔºâËá™Âä®Êé®ÈÄÅ„ÄÇ
  ÂÜÖÂÆπÂàÜ‰∏∫„Äå‰ªäÊó•ÂøÖÁúã„Äç„ÄåÊé®Ëçê„Äç„ÄåÂÖ∂‰ªñ„Äç‰∏â‰∏™ÊùøÂùó„ÄÇ
  AI Ê†πÊçÆ‰Ω†ÁöÑÂÅèÂ•ΩÊô∫ËÉΩÁ≠õÈÄâ 15-30 Êù°Á≤æÈÄâÂÜÖÂÆπ„ÄÇ

ÂÅèÂ•ΩËÆæÁΩÆ (/settings)
  Êü•ÁúãÊàñÊõ¥Êñ∞‰Ω†ÁöÑ Web3 ÂÖ¥Ë∂£ÂÅèÂ•Ω„ÄÇ
  AI ‰ºöÊ†πÊçÆÂÅèÂ•Ω‰∏™ÊÄßÂåñÁ≠õÈÄâÊØèÊó•ÁÆÄÊä•„ÄÇ

‰ø°ÊÅØÊ∫êÁÆ°ÁêÜ (/sources)
  Ê∑ªÂä†/Âà†Èô§‰Ω†ÂÖ≥Ê≥®ÁöÑ Twitter Ë¥¶Âè∑ÊàñÁΩëÁ´ô RSS„ÄÇ
  ÊîØÊåÅËá™ÂÆö‰πâ‰∏™‰∫∫‰ø°ÊÅØÊ∫ê„ÄÇ

ÁªüËÆ° (/stats)
  Êü•Áúã‰Ω†ÁöÑÊ≥®ÂÜåÊó∂Èó¥„ÄÅÂèçÈ¶àÂéÜÂè≤„ÄÅÊª°ÊÑèÂ∫¶Ë∂ãÂäø„ÄÇ

ÂèçÈ¶àÊú∫Âà∂
  ÊØèÊù°Êé®ÈÄÅÊ∂àÊÅØÈÉΩÊúâÂèçÈ¶àÊåâÈíÆÔºàüëç/üëéÔºâ„ÄÇ
  ‰Ω†ÁöÑÂèçÈ¶à‰ºöË¢´Êî∂ÈõÜÂπ∂Âú®ÊØèÊó•ÂáåÊô®ÊâπÈáèÊõ¥Êñ∞ÂÅèÂ•ΩÁîªÂÉèÔºå
  Ê¨°Êó•Êé®ÈÄÅÂ∞Ü‰ΩìÁé∞‰Ω†ÁöÑÊúÄÊñ∞ÂÅèÂ•Ω„ÄÇ

{'‚îÄ' * 24}

ÊúâÈóÆÈ¢òÔºü‰ΩøÁî®‰∏äÊñπÂëΩ‰ª§Êàñ‰∏ªËèúÂçïÊìç‰Ωú„ÄÇ"""

    await query.edit_message_text(help_text, reply_markup=reply_markup)


def main() -> None:
    """Main function to run the bot."""
    if not TELEGRAM_BOT_TOKEN:
        logger.error("TELEGRAM_BOT_TOKEN not set. Please check your .env file.")
        sys.exit(1)

    # Build application
    application = (
        Application.builder()
        .token(TELEGRAM_BOT_TOKEN)
        .post_init(post_init)
        .build()
    )

    # Add handlers
    
    # 1. Admin handlers (Priority: Highest - always handle admin commands first)
    for handler in get_admin_handlers():
        application.add_handler(handler)
    logger.info("Admin handlers registered")

    # 2. Start/onboarding conversation handler
    application.add_handler(get_start_handler())

    # Start menu callbacks
    for callback in get_start_callbacks():
        application.add_handler(callback)

    # Settings handler
    application.add_handler(get_settings_handler())
    for callback in get_settings_callbacks():
        application.add_handler(callback)

    # Sources handler
    application.add_handler(get_sources_handler())
    for callback in get_sources_callbacks():
        application.add_handler(callback)

    # Feedback handlers
    feedback_conv, item_handler = get_feedback_handlers()
    application.add_handler(feedback_conv)
    application.add_handler(item_handler)

    # Command handlers
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("test", test_fetch_command))
    application.add_handler(CommandHandler("testprofile", test_profile_command))
    application.add_handler(CommandHandler("testprefetch", test_prefetch_command))
    application.add_handler(CommandHandler("stats", stats_command))

    # Callback for help from unknown message
    application.add_handler(CallbackQueryHandler(show_help_callback, pattern="^show_help$"))
    application.add_handler(CallbackQueryHandler(noop_callback, pattern="^noop$"))

    # Error handler
    application.add_error_handler(error_handler)

    # Register shutdown handler to save shutdown log
    # ÊúçÂä°ÂÖ≥Èó≠Êó∂‰øùÂ≠ò shutdown Êó•Âøó
    def on_shutdown():
        shutdown_time = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        shutdown_log_path = os.path.join(LOGS_DIR, f"bot.log.shutdown.{shutdown_time}")
        try:
            # Copy current log to shutdown file
            current_log_path = os.path.join(LOGS_DIR, "bot.log")
            if os.path.exists(current_log_path):
                import shutil
                shutil.copy2(current_log_path, shutdown_log_path)
                logger.info(f"Shutdown log saved to {shutdown_log_path}")
        except Exception as e:
            logger.error(f"Failed to save shutdown log: {e}")
        logger.info("Bot shutting down...")

    atexit.register(on_shutdown)

    # Start the bot
    logger.info(f"Admin IDs: {len(ADMIN_TELEGRAM_IDS)} configured")
    logger.info("Starting Web3 Daily Digest Bot...")
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
